{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning A-Z Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 1_Data Preprocessing\n",
    "\n",
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "\n",
    "dataset = pd.read_csv('Data.csv')\n",
    "print(dataset)\n",
    "print('------------------------------------')\n",
    "\n",
    "# Create matrix of independent and dependent variable \n",
    "X = dataset.iloc[:,:-1].values # independent variables\n",
    "y = dataset.iloc[:,-1].values # dependent variables\n",
    "X.ndim, y.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependent variable is 'Purchased' here. Rest are independent variables. Machine Learning (ML) will use the independent variables to predict the dependent variable.\n",
    "\n",
    "* Dependent variable: Purchased\n",
    "* Independent variable: Age, Salary, Purchased\n",
    "\n",
    "Data preprocessing is common initial step before starting ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing Data\n",
    "\n",
    "# Strategy 1: Remove observation that contains NaN \n",
    "# Strategy 2: Replace Nan by mean of relevant column/row\n",
    "\n",
    "from sklearn.preprocessing import Imputer # Imputer class\n",
    "\n",
    "# as we imported the class, we need to create an object of the class\n",
    "imputer = Imputer(missing_values = 'NaN', \n",
    "                  strategy= 'mean', axis=0) # Imputer object\n",
    "\n",
    "# Fit imputer to matrix X (matrix X has missing data)\n",
    "imputer = imputer.fit(X[:,1:]) \n",
    "X[:,1:] = imputer.transform(X[:,1:]) #conversion of matrix X based on imputer\n",
    "X # NaN has been replaced with mean values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 categorical variables in the dataset Country (Independent) and\n",
    "Purchased (dependent). Country variable contains 3 categories and \n",
    "Purchase variable contains 2 categories. We want ONLY numbers for ML and\n",
    "not text (like in these categorical features). Thus we need to convert the \n",
    "categories into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 44.0, 72000.0],\n",
       "       [2, 27.0, 48000.0],\n",
       "       [1, 30.0, 54000.0],\n",
       "       [2, 38.0, 61000.0],\n",
       "       [1, 40.0, 63777.77777777778],\n",
       "       [0, 35.0, 58000.0],\n",
       "       [2, 38.77777777777778, 52000.0],\n",
       "       [0, 48.0, 79000.0],\n",
       "       [1, 50.0, 83000.0],\n",
       "       [0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorical variables\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "labelencoder_X=LabelEncoder() # creating labelencoder object\n",
    "# X[:,0] is categorical\n",
    "X[:,0]=labelencoder_X.fit_transform(X[:,0])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the country (categorical) column is converted into numerical. \n",
    "\n",
    "**But**, 3 categories of country column are converted into 0, 1, 2. Since 0 <1 <2, equations in the model will think that 1 country is bigger than other based on the number representing them. ML will think Spain is bigger than Germany as 2>0. BUT there is **no relational order between the countries**. \n",
    "\n",
    "Label encoder encode values without thinking whether there is order or not!\n",
    "\n",
    "But if we have 3 categories in the same column such as small, medium and large. Then 0,1,2 makes sense as **there is an order between small, medium and large**. But in country there is no order so we have to use something other than `LabelEncoder`.\n",
    "\n",
    "### Thus, we have to use  Dummy encoding in the case of country column.\n",
    "Dummy encoding will convert 1 country column into 3 columns because \n",
    "there are 3 categories. Each dummy variable column \n",
    "will represent 1 country. We will need OneHotEncoder for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0e+00, 0.0e+00, 0.0e+00, 4.4e+01, 7.2e+04])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehotencoder= OneHotEncoder(categorical_features=[0])\n",
    "X=onehotencoder.fit_transform(X).toarray()\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously X had 3 columns. Now country column is replaced by 3 dummy variables or columns.\n",
    "So 3-1+3=5 columns. So OneHotEncoder worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For y or target/dependent variable LabelEncoder will work fine as y has binary category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder_y=LabelEncoder()\n",
    "y=labelencoder_y.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into Training and Test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, \n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to be careful about ML algorithem understanding the correlation between X_train and\n",
    "y_train. If they don't understand the logic between X_train and y_train and just learn it by heart, \n",
    "it will have trouble predicting X_test. This is the case where overfitting of data happens. But there are \n",
    "regularization techniques to prevent this overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "#### Required because scale of features are uneven and widely varied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally feature scaling is extremely necessary: Age range and salary range are widely different.\n",
    "\n",
    "\n",
    "ML model accuracy also depends on euclidean distance between data points. If there are 2 points, then they calculate\n",
    "euclidean distance by ((y2-y1)^2+(x2-x1)^2)^1/2. Now one variable (suppose y) here salary\n",
    "has a way higher scale and it is squared (which amplifies it further) and added with the square of variable x (Age in this case. Age \n",
    "value is negligible even after squaring). Then ML will only consider y value (Salary here) as it has a higher magnitude in comparison to age (thus effect of age won't be considered). We need to normalize that. Thats where feature scaling is really helpful.\n",
    "\n",
    "Feature scaling can be done by standardization\n",
    "\n",
    "1. `STANDARDIZATION: x_standardization= x - mean(x) / standard deviation(x)`\n",
    "\n",
    "2. `NORMALIZATION: x_normalization= x - min(x) / max(x) - min(x)`\n",
    "\n",
    "So we need to standardize them such that each variable ranges from -1 to +1. \n",
    "And thus their high range wont affect ML model to be biased\n",
    "\n",
    "\n",
    "**CODE:**\n",
    "\n",
    "`from sklearn.preprocessing import StandardScaler`\n",
    "\n",
    "`sc_X = StandardScaler()`\n",
    "\n",
    "`X_train = sc_X.fit_transform(X_train)`\n",
    "\n",
    "`X_test = sc_X.transform(X_test)`\n",
    "\n",
    "If we have to standardize y, add the following code to the above code--\n",
    "\n",
    "`sc_y = StandardScaler()`\n",
    "\n",
    "`y_train = sc.y.fit_transform(y_train)`\n",
    "\n",
    "For test set, we don't need to fit the sc_X instance because it's already fitted to training set\n",
    "That means X_test is transformed by StandardScaler based on its fitting to the X_train. So \n",
    "both X_train and X_test are fitted based on the same scale.\n",
    "\n",
    "Here dependent variable or y is categorical as it has 1 or 0 values, we do NOT need to use feature scaling for clasification.\n",
    "\n",
    "But for regression where dependent variable or y can take a huge range\n",
    "(unlike classification), then we need to use feature scaling for y as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler # StandardScaler class\n",
    "sc_X = StandardScaler() # creating sc_x, a StandardScaler object\n",
    "\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.          2.64575131 -0.77459667  0.26306757  0.12381479]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.25350148  0.46175632]\n",
      " [-1.         -0.37796447  1.29099445 -1.97539832 -1.53093341]\n",
      " [-1.         -0.37796447  1.29099445  0.05261351 -1.11141978]\n",
      " [ 1.         -0.37796447 -0.77459667  1.64058505  1.7202972 ]\n",
      " [-1.         -0.37796447  1.29099445 -0.0813118  -0.16751412]\n",
      " [ 1.         -0.37796447 -0.77459667  0.95182631  0.98614835]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.59788085 -0.48214934]]\n",
      "--------------------------------------------------------------\n",
      "[[-1.          2.64575131 -0.77459667 -1.45882927 -0.90166297]\n",
      " [-1.          2.64575131 -0.77459667  1.98496442  2.13981082]]\n",
      "--------------------------------------------------------------\n",
      "[1 1 1 0 1 0 0 1]\n",
      "--------------------------------------------------------------\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train) # numbers range between -1 to +1 because of feature scaling\n",
    "print('--------------------------------------------------------------')\n",
    "print(X_test) # numbers range between -1 to +1 because of feature scaling\n",
    "print('--------------------------------------------------------------')\n",
    "print(y_train) # No feature scaling for classification \n",
    "print('--------------------------------------------------------------')\n",
    "print(y_test) # No feature scaling for classification "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
