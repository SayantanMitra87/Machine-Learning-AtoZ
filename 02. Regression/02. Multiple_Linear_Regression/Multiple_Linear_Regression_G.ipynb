{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning A-Z Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Multiple Linear Regression (Continuous regression)\n",
    "\n",
    "### Criteria: Linear regression problems with multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simple linear regression (Previous notebook): 1 independent/predictor variable\n",
    "\n",
    "\n",
    "    \n",
    "* Multiple linear regression: Multiple independent/predictor variable\n",
    "\n",
    "Here we are hired as the data scientist for the VC firm who wants to figure out rationale behind more profit. Therefore, dependent variable is profit. We are trying to understand what feature/s ensure higher profit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0  165349.20       136897.80        471784.10    New York  192261.83\n",
       "1  162597.70       151377.59        443898.53  California  191792.06\n",
       "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3  144372.41       118671.85        383199.62    New York  182901.99\n",
       "4  142107.34        91391.77        366168.42     Florida  166187.94"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset=pd.read_csv('50_Startups.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   R&D Spend  Administration  Marketing Spend       State\n",
      "0  165349.20       136897.80        471784.10    New York\n",
      "1  162597.70       151377.59        443898.53  California\n",
      "2  153441.51       101145.55        407934.54     Florida\n",
      "3  144372.41       118671.85        383199.62    New York\n",
      "4  142107.34        91391.77        366168.42     Florida\n",
      "------------------------------------\n",
      "(50, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=dataset.iloc[:,:-1] # all independent variables\n",
    "print(X.head())\n",
    "print('------------------------------------')\n",
    "print(X.shape)\n",
    "X['State'].nunique() # 3 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    192261.83\n",
      "1    191792.06\n",
      "2    191050.39\n",
      "3    182901.99\n",
      "4    166187.94\n",
      "Name: Profit, dtype: float64\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=dataset.iloc[:,-1] # dependent | target variable\n",
    "print(y.head())\n",
    "print('------------------------------------')\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Independent variable: All except Profit | Dependent variable: Profit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[165349.2 136897.8 471784.1 'New York']\n",
      " [162597.7 151377.59 443898.53 'California']\n",
      " [153441.51 101145.55 407934.54 'Florida']\n",
      " [144372.41 118671.85 383199.62 'New York']\n",
      " [142107.34 91391.77 366168.42 'Florida']]\n",
      "------------------------------------\n",
      "[192261.83 191792.06 191050.39 182901.99 166187.94]\n",
      "------------------------------------\n",
      "(50, 4) (50,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=dataset.iloc[:,:-1].values # matrix of all independent variables\n",
    "y=dataset.iloc[:,-1].values # dependent variable vector\n",
    "print(X[:5]) \n",
    "print('------------------------------------')\n",
    "print(y[:5]) \n",
    "print('------------------------------------')\n",
    "print(X.shape, y.shape)\n",
    "X.ndim, y.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables need to be converted into numbers for ML: Salary column has 3 categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical variables can be converted into numerical values by **LabelEncoder**. But it might lead to weightage biasness.\n",
    "\n",
    "Then to remove any weightage biasness we will convert that numerical column (made from \n",
    "categorical column) into DUMMY variables by **OneHotEncoder**. OneHotEncoder only work on \n",
    "numerical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[165349.2 136897.8 471784.1 'New York']\n",
      " [162597.7 151377.59 443898.53 'California']\n",
      " [153441.51 101145.55 407934.54 'Florida']\n",
      " [144372.41 118671.85 383199.62 'New York']\n",
      " [142107.34 91391.77 366168.42 'Florida']]\n",
      "---------------------------------------\n",
      "[[165349.2 136897.8 471784.1 2]\n",
      " [162597.7 151377.59 443898.53 0]\n",
      " [153441.51 101145.55 407934.54 1]\n",
      " [144372.41 118671.85 383199.62 2]\n",
      " [142107.34 91391.77 366168.42 1]]\n",
      "---------------------------------------\n",
      "[2 0 1 2 1 2 0 1 2 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "labelencoder_X=LabelEncoder()\n",
    "print(X[:5]) # before using labelencoder\n",
    "print('---------------------------------------')\n",
    "\n",
    "X[:,-1]=labelencoder_X.fit_transform(X[:,-1])\n",
    "print(X[:5]) # after using LabelEncoder\n",
    "print('---------------------------------------')\n",
    "\n",
    "print(X[:15,3]) # State converting into numerical column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although above code helped to remove state name but it got replaced with 0, 1, 2 and it will lead to weightage biasness. (example-ML will think 1 state is bigger than other as 2>0.) \n",
    "\n",
    "To prevent this use **'Dummy variable'**. Make n number of columns based on n number of categories in a column. And each column will have 1 or 0 based on whether it is present or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 0.0000000e+00 1.0000000e+00 1.6534920e+05 1.3689780e+05\n",
      "  4.7178410e+05]\n",
      " [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6259770e+05 1.5137759e+05\n",
      "  4.4389853e+05]]\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50, 6)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehotencoder= OneHotEncoder(categorical_features=[-1])\n",
    "X=onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "print(X[:2,:]) \n",
    "print('---------------------------------------')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously there were 4 columns in X. After 1 OneHotEncoder last column with 3 categories is converted into 3 columns. so total column will be (4-1)+3=6 columns. Remove 1 of the columns generated by dummy variable, which is redundant (inputs in 2 columns will tell us input in the 3rd column), which will lead to total 5 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 5)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avoiding Dummy Variable TRAP!\n",
    "X=X[:,1:] # Remove 1 dummy variable column to prevent dummy variable trap!\n",
    "\n",
    "X.shape # X(50,6) ---> (50,5) confirming removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the dataset into the Training and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, \n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Multiple Linear Regression to the Training set\n",
    "from sklearn.linear_model import LinearRegression # LinearRegression class\n",
    "\n",
    "regressor = LinearRegression() # Regressor is the object of LinearRegression class\n",
    "\n",
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([103015.20159796, 132582.27760816, 132447.73845175,  71976.09851258,\n",
       "       178537.48221056, 116161.24230165,  67851.69209676,  98791.73374687,\n",
       "       113969.43533013, 167921.06569551])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't visualize here unlike simple linear regression as we need 5 dimensions because 4 Independent & 1 dependent variable. So we predicted based on the model. But do we need to use all the independent variables for the model building and prediction? Or could we find out the independent variables that have more impact on the dependent variable! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the optimal model using Backward Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use all independent variables to predict the target variable. But there might be some\n",
    "independent variable that are statistically more significant i.e. they have more impact on dependent\n",
    "variable or Profit. And some are statistically insignificant and have low/no impact on Profit.\n",
    "\n",
    "And we will still get great prediction even if we remove the statistically \n",
    "insignificant variables.\n",
    "\n",
    "#### Overall goal: to find an optimal team of independent variable, that has a great impact on dependent variable (Profit) and thus they are great predictors for dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiple linear regression equation, there is `bo` which is a constant, but there is no\n",
    "`Xo`(independent variable) associated with `bo` unlike seen in the case of all independent variables (eg- `X1` with `b1`, \n",
    "`X2` with `b2`). Linear Regression from sklearn considers `bo`, but statsmodel DO NOT consider `bo`. But we still need to use statsmodel as we need to calculate `p-value` and statistical significance of the independent variables.\n",
    "Therefore, we can add a new column `Xo` where all of its value will be 1 i.e. `Xo=1`.\n",
    "Because if `Xo=1` then `boXo = bo`. So `Xo=1` will NOT affect `bo` value.\n",
    "\n",
    "PROBLEM: The statsmodels we are using do NOT consider `bo`. That's a problem!\n",
    "We see matrix X contains independent features and dummy variables. \n",
    "There is no `Xo` for `bo`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In sklearn.linear_model, bo is included but NOT in the case of statsmodels. \n",
    "#### Solution: Take X matrix and add a column (contianing value 1 and can act as Xo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 6)\n",
      "---------------------------------------\n",
      "[[1.0000000e+00 0.0000000e+00 1.0000000e+00 1.6534920e+05 1.3689780e+05\n",
      "  4.7178410e+05]\n",
      " [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6259770e+05 1.5137759e+05\n",
      "  4.4389853e+05]]\n"
     ]
    }
   ],
   "source": [
    "X = np.append(arr = np.ones((50,1)).astype(int), values = X, axis = 1)\n",
    "\n",
    "print(X.shape)\n",
    "# X(50,5)-->(50,6) confirming addition of Xo column with constant value 1\n",
    "print('---------------------------------------')\n",
    "print(X[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`X = np.append(arr = X,  values = np.ones((50,1)).astype(int),axis = 1)`** - This will add the new array containing 1 at the end of the matrix X.\n",
    "\n",
    "To add the array at the beginning of the matrix X, `swap arr and values`, i.e.\n",
    "**`X = np.append(arr = np.ones((50,1)).astype(int), values = X, axis = 1)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a new matrix of features called **optimal matrix of features** and we \n",
    "will name it **X_opt**. The team of independent variables which will make `X_opt` will be \n",
    "statistically significant (have higher impact) for predicting dependent variable (Profit here).\n",
    "\n",
    "\n",
    "Here we will follow backward elimination. Therefore we include all independent variables \n",
    "and then we replace one independent variable at a time if their \n",
    "     `p value > SL` (generally Significance Level is 0.05 or 5%), one with the highest p value will be removed. \n",
    "That means overall, independent variables that are NOT statistically significant will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 6)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For backward elimination, X_opt initially contains all independent variables\n",
    "X_opt = X[:,[0,1,2,3,4,5]] \n",
    "\n",
    "\n",
    "X_opt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Elimination (Fit full model with all possible predictors)\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "\n",
    "# OLS = Ordinary least square fitted in the X_opt and dependent variable\n",
    "# OLS is the class. regressor_OLS is the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under OLS documentation, it is mentioned in 'exog' it don't include an intercept and that's why we had \n",
    "to append a column with values 1 to make it `Xo` in last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.951\n",
      "Model:                            OLS   Adj. R-squared:                  0.945\n",
      "Method:                 Least Squares   F-statistic:                     169.9\n",
      "Date:                Tue, 23 Oct 2018   Prob (F-statistic):           1.34e-27\n",
      "Time:                        14:50:52   Log-Likelihood:                -525.38\n",
      "No. Observations:                  50   AIC:                             1063.\n",
      "Df Residuals:                      44   BIC:                             1074.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       5.013e+04   6884.820      7.281      0.000    3.62e+04     6.4e+04\n",
      "x1           198.7888   3371.007      0.059      0.953   -6595.030    6992.607\n",
      "x2           -41.8870   3256.039     -0.013      0.990   -6604.003    6520.229\n",
      "x3             0.8060      0.046     17.369      0.000       0.712       0.900\n",
      "x4            -0.0270      0.052     -0.517      0.608      -0.132       0.078\n",
      "x5             0.0270      0.017      1.574      0.123      -0.008       0.062\n",
      "==============================================================================\n",
      "Omnibus:                       14.782   Durbin-Watson:                   1.283\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.266\n",
      "Skew:                          -0.948   Prob(JB):                     2.41e-05\n",
      "Kurtosis:                       5.572   Cond. No.                     1.45e+06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.45e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "print(regressor_OLS.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary function will help us to know p value for each Independent variables. Then we will \n",
    "compare it to the significance level (SL) to decide whether it needs to be removed \n",
    "or not from the model**\n",
    "\n",
    "* Consider predictor or Independent variable with highest P value (P).\n",
    "\n",
    "* If it's `P > Significance Level` --> remove predictor, otherwise Model finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULT:** Now look into the **P values** `(P > |t|)`, lower the P value more statistically \n",
    "significant Independent variable is going to be. \n",
    "\n",
    "Constant **(const)** is `Xo`, that we added to the X (array of 1).\n",
    "Then `X1`, `X2` are dummary variables, `X3` is R&D spend, `X4` is administration spend,\n",
    "`X5` is marketing spend. Now we have to look for highest p-Value. \n",
    "\n",
    "`X2` has highest P value i.e. 99%. That is WAY above Significance Level of 5%, so `X2` has to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 5)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove X2 \n",
    "X_opt = X[:,[0,1,3,4,5]] \n",
    "\n",
    "# X_opt now contains Independent variables without predictor X2\n",
    "X_opt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.951\n",
      "Model:                            OLS   Adj. R-squared:                  0.946\n",
      "Method:                 Least Squares   F-statistic:                     217.2\n",
      "Date:                Tue, 23 Oct 2018   Prob (F-statistic):           8.49e-29\n",
      "Time:                        14:50:52   Log-Likelihood:                -525.38\n",
      "No. Observations:                  50   AIC:                             1061.\n",
      "Df Residuals:                      45   BIC:                             1070.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       5.011e+04   6647.870      7.537      0.000    3.67e+04    6.35e+04\n",
      "x1           220.1585   2900.536      0.076      0.940   -5621.821    6062.138\n",
      "x2             0.8060      0.046     17.606      0.000       0.714       0.898\n",
      "x3            -0.0270      0.052     -0.523      0.604      -0.131       0.077\n",
      "x4             0.0270      0.017      1.592      0.118      -0.007       0.061\n",
      "==============================================================================\n",
      "Omnibus:                       14.758   Durbin-Watson:                   1.282\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.172\n",
      "Skew:                          -0.948   Prob(JB):                     2.53e-05\n",
      "Kurtosis:                       5.563   Cond. No.                     1.40e+06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.4e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Fit full model with rest predictors again\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit() \n",
    "print(regressor_OLS.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Result:** `X1` has highest P value i.e. 94%. That is WAY above SL of 5%, so X1 has to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4)\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.951\n",
      "Model:                            OLS   Adj. R-squared:                  0.948\n",
      "Method:                 Least Squares   F-statistic:                     296.0\n",
      "Date:                Tue, 23 Oct 2018   Prob (F-statistic):           4.53e-30\n",
      "Time:                        14:50:52   Log-Likelihood:                -525.39\n",
      "No. Observations:                  50   AIC:                             1059.\n",
      "Df Residuals:                      46   BIC:                             1066.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       5.012e+04   6572.353      7.626      0.000    3.69e+04    6.34e+04\n",
      "x1             0.8057      0.045     17.846      0.000       0.715       0.897\n",
      "x2            -0.0268      0.051     -0.526      0.602      -0.130       0.076\n",
      "x3             0.0272      0.016      1.655      0.105      -0.006       0.060\n",
      "==============================================================================\n",
      "Omnibus:                       14.838   Durbin-Watson:                   1.282\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.442\n",
      "Skew:                          -0.949   Prob(JB):                     2.21e-05\n",
      "Kurtosis:                       5.586   Cond. No.                     1.40e+06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.4e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Remove X1 (predictor) and again fit model with non-eliminated predictors\n",
    "X_opt = X[:,[0,3,4,5]]\n",
    "print(X_opt.shape)\n",
    "\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit() \n",
    "print(regressor_OLS.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULT:** `X2`(Admin. expense) has highest P value i.e. 60%. That is above SL of 5%, so `X2` has to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 3)\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.950\n",
      "Model:                            OLS   Adj. R-squared:                  0.948\n",
      "Method:                 Least Squares   F-statistic:                     450.8\n",
      "Date:                Tue, 23 Oct 2018   Prob (F-statistic):           2.16e-31\n",
      "Time:                        14:50:52   Log-Likelihood:                -525.54\n",
      "No. Observations:                  50   AIC:                             1057.\n",
      "Df Residuals:                      47   BIC:                             1063.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\n",
      "x1             0.7966      0.041     19.266      0.000       0.713       0.880\n",
      "x2             0.0299      0.016      1.927      0.060      -0.001       0.061\n",
      "==============================================================================\n",
      "Omnibus:                       14.677   Durbin-Watson:                   1.257\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\n",
      "Skew:                          -0.939   Prob(JB):                     2.54e-05\n",
      "Kurtosis:                       5.575   Cond. No.                     5.32e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.32e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Remove X2 (predictor) and Again fit model with non-eliminated predictor\n",
    "X_opt = X[:,[0,3,5]]\n",
    "print(X_opt.shape)\n",
    "\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit() \n",
    "print(regressor_OLS.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULT:** `X2` has highest P value i.e. 6%. That is above SL of 5%, so `X2` has to be removed.\n",
    "\n",
    "**Note:** We can also set SL at 10%, then `X2` would be included. But if we have to follow\n",
    "Backward elimination framework which has SL of 5%, then `X2` has to be eliminated.\n",
    "\n",
    "But as 6% is very close to 5%(significance level), we can also confirm Marketing spend should be eliminated or not looking into other factors such as R squared and adjusted R squared. Those will be covered in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.947\n",
      "Model:                            OLS   Adj. R-squared:                  0.945\n",
      "Method:                 Least Squares   F-statistic:                     849.8\n",
      "Date:                Tue, 23 Oct 2018   Prob (F-statistic):           3.50e-32\n",
      "Time:                        14:50:52   Log-Likelihood:                -527.44\n",
      "No. Observations:                  50   AIC:                             1059.\n",
      "Df Residuals:                      48   BIC:                             1063.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       4.903e+04   2537.897     19.320      0.000    4.39e+04    5.41e+04\n",
      "x1             0.8543      0.029     29.151      0.000       0.795       0.913\n",
      "==============================================================================\n",
      "Omnibus:                       13.727   Durbin-Watson:                   1.116\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               18.536\n",
      "Skew:                          -0.911   Prob(JB):                     9.44e-05\n",
      "Kurtosis:                       5.361   Cond. No.                     1.65e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.65e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Remove X2 (predictor) and again fit model with non-eliminated predictor\n",
    "X_opt = X[:,[0,3]]\n",
    "print(X_opt.shape)\n",
    "\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit() \n",
    "print(regressor_OLS.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULT:** **None has more than 5%**\n",
    "\n",
    "**Therefore, X1 i.e. R&D spend is a powerful predictor of profit (dependent variable).**\n",
    "Therefore, X_opt instead of having a team of independent variables (predictor variables) will only contain 1 independent variable i.e. R&D spend (when just following backward elimination strictly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note 46: Automated Backward Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Elimination with p-values only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.950\n",
      "Model:                            OLS   Adj. R-squared:                  0.948\n",
      "Method:                 Least Squares   F-statistic:                     450.8\n",
      "Date:                Tue, 23 Oct 2018   Prob (F-statistic):           2.16e-31\n",
      "Time:                        14:51:12   Log-Likelihood:                -525.54\n",
      "No. Observations:                  50   AIC:                             1057.\n",
      "Df Residuals:                      47   BIC:                             1063.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\n",
      "x1             0.7966      0.041     19.266      0.000       0.713       0.880\n",
      "x2             0.0299      0.016      1.927      0.060      -0.001       0.061\n",
      "==============================================================================\n",
      "Omnibus:                       14.677   Durbin-Watson:                   1.257\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\n",
      "Skew:                          -0.939   Prob(JB):                     2.54e-05\n",
      "Kurtosis:                       5.575   Cond. No.                     5.32e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.32e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.formula.api import OLS\n",
    "\n",
    "\n",
    "def backwardelimination(x_opt,sl):\n",
    "\n",
    "    model_ols=OLS(y,x_opt).fit()\n",
    "    p=max(model_ols.pvalues).astype(float)\n",
    "\n",
    "    if p>sl:\n",
    "        pos=0\n",
    "        while model_ols.pvalues[pos].astype(float)!=p:\n",
    "            pos=pos+1\n",
    "            x_opt=np.delete(x_opt,pos,1)\n",
    "\n",
    "        return backwardelimination(x_opt,sl)\n",
    "\n",
    "    else:\n",
    "        return x_opt\n",
    "\n",
    "\n",
    "    \n",
    "SL = 0.05\n",
    "X_opt = X[:, [0, 1, 2, 3, 4, 5]]\n",
    "X_Modeled = backwardElimination(X_opt, SL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Elimination with p-values and Adjusted R Squared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.950\n",
      "Model:                            OLS   Adj. R-squared:                  0.948\n",
      "Method:                 Least Squares   F-statistic:                     450.8\n",
      "Date:                Tue, 23 Oct 2018   Prob (F-statistic):           2.16e-31\n",
      "Time:                        14:51:07   Log-Likelihood:                -525.54\n",
      "No. Observations:                  50   AIC:                             1057.\n",
      "Df Residuals:                      47   BIC:                             1063.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\n",
      "x1             0.7966      0.041     19.266      0.000       0.713       0.880\n",
      "x2             0.0299      0.016      1.927      0.060      -0.001       0.061\n",
      "==============================================================================\n",
      "Omnibus:                       14.677   Durbin-Watson:                   1.257\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\n",
      "Skew:                          -0.939   Prob(JB):                     2.54e-05\n",
      "Kurtosis:                       5.575   Cond. No.                     5.32e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.32e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "def backwardElimination(x, SL):\n",
    "    numVars = len(x[0])\n",
    "    temp = np.zeros((50,6)).astype(int)\n",
    "    for i in range(0, numVars):\n",
    "        regressor_OLS = sm.OLS(y, x).fit()\n",
    "        maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "        adjR_before = regressor_OLS.rsquared_adj.astype(float)\n",
    "        if maxVar > SL:\n",
    "            for j in range(0, numVars - i):\n",
    "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                    temp[:,j] = x[:, j]\n",
    "                    x = np.delete(x, j, 1)\n",
    "                    tmp_regressor = sm.OLS(y, x).fit()\n",
    "                    adjR_after = tmp_regressor.rsquared_adj.astype(float)\n",
    "                    if (adjR_before >= adjR_after):\n",
    "                        x_rollback = np.hstack((x, temp[:,[0,j]]))\n",
    "                        x_rollback = np.delete(x_rollback, j, 1)\n",
    "                        print (regressor_OLS.summary())\n",
    "                        return x_rollback\n",
    "                    else:\n",
    "                        continue\n",
    "    regressor_OLS.summary()\n",
    "    return x\n",
    " \n",
    "SL = 0.05\n",
    "X_opt = X[:, [0, 1, 2, 3, 4, 5]]\n",
    "X_Modeled = backwardElimination(X_opt, SL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
